{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62907bd9-5906-4409-bae0-cca720b488a2",
   "metadata": {},
   "source": [
    "<h2 style = 'color:purple;'><u>Note to rememeber :</u>Here gradient descent is not only applied to linear regression but also in other algorithm like logistic regression or deep learning.We have to use this formula in all b_new = b_old - n*slope,here only slope is different in all.Slope is a derivative of a loss function with respect to b in all case.So here in linear regression we differentiate slope w.r.t b and get slope as -2*np.sum(Y - m*X.ravel() - b).In other algorithm too we have to differentiate their loss function and calculate their slope.Only slope is different in formula b_new = b_old - n*m while applying gradient descent in other algorithm</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e8620be0-48bb-42b5-b517-d1c8904a1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7bcaec8c-38ac-455b-89cc-57ad4eb42fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y =make_regression(n_samples=20, n_features=1, n_informative=1, n_targets=1,noise=80,random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "56ee0414-fb8f-4255-8312-502c20e9812f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.04450308],\n",
       "        [-0.78898902],\n",
       "        [ 0.53233789],\n",
       "        [ 0.9137407 ],\n",
       "        [ 0.75376638],\n",
       "        [-1.04537713],\n",
       "        [ 0.86121137],\n",
       "        [ 0.12730328],\n",
       "        [ 1.47868574],\n",
       "        [ 1.3501879 ],\n",
       "        [-0.24332625],\n",
       "        [-1.26160595],\n",
       "        [ 0.56284679],\n",
       "        [ 0.31735092],\n",
       "        [-0.71239066],\n",
       "        [ 1.34510171],\n",
       "        [-0.02677165],\n",
       "        [ 2.15038297],\n",
       "        [ 0.60628866],\n",
       "        [ 0.45181234]]),\n",
       " array([ -94.65916603,  109.63621212,   19.04994344,  -88.88645663,\n",
       "         -78.94743471, -111.19040099, -128.07635379,  -19.35784681,\n",
       "         -54.31575284,  -21.00717628,  -36.27048044,   42.07716836,\n",
       "          15.62116804,   11.84998644,   66.22051641,   40.79621837,\n",
       "          60.87478741,  -30.52448303,  -88.66957067,  -74.45970822]))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X ,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "87388f9c-192d-4f84-8749-b695ca1196e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "caa97c51-5835-4626-8bdb-1e5bfdbd9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train,Y_train)\n",
    "Y_pred = lr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c323c720-2625-463c-bd9d-8ef891f912b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2943540700748284"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(Y_test,Y_pred)\n",
    "# Y_pred.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7499f30a-afb0-46f3-a34d-58d45ab81038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-28.08207829])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_ ### value of slope i.e. m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c6d7ac10-0222-4600-bf7e-07ce1d836fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-22.153433743184706"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_  ### Value of b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7c356d2e-c4a5-4c61-a7f8-68f6f9f0746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class MeroRegression:\n",
    "    def __init__(self,learning_rate,epoch):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epoch\n",
    "        self.m = 100          #Taking a random value for m\n",
    "        self.b = -120         #Taking a random value for b \n",
    "    def fit(self,X,Y):\n",
    "        for i in range(self.epoch):\n",
    "            loss_slope_b = -2 * np.sum(Y - self.m * X.ravel() - self.b)\n",
    "            loss_slope_m = -2 * np.sum((Y - self.m * X.ravel() - self.b)*X.ravel())\n",
    "            self.b = self.b - (self.learning_rate * loss_slope_b)\n",
    "            self.m = self.m - (self.learning_rate * loss_slope_m)\n",
    "            print('Iteration : ',i)\n",
    "            print(self.m)\n",
    "            print(self.b)\n",
    "        print(self.m)\n",
    "        print(self.b)\n",
    "    def predict(self,X):\n",
    "        return self.m*X.ravel() + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "08e2a8ba-4350-44a9-9824-21d254c7b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  0\n",
      "81.73441416062579\n",
      "-100.64187288023683\n",
      "Iteration :  1\n",
      "65.56931177197029\n",
      "-85.77377993002428\n",
      "Iteration :  2\n",
      "51.47441501187082\n",
      "-74.15492986505409\n",
      "Iteration :  3\n",
      "39.31013034198281\n",
      "-64.9387590491009\n",
      "Iteration :  4\n",
      "28.887720929781153\n",
      "-57.53657717210729\n",
      "Iteration :  5\n",
      "20.003883444865167\n",
      "-51.53046167430363\n",
      "Iteration :  6\n",
      "12.459800084089121\n",
      "-46.61725270295901\n",
      "Iteration :  7\n",
      "6.070888911957818\n",
      "-42.57224764094431\n",
      "Iteration :  8\n",
      "0.6710805336569123\n",
      "-39.22542329968502\n",
      "Iteration :  9\n",
      "-3.886038985550403\n",
      "-36.44566611465625\n",
      "Iteration :  10\n",
      "-7.727801328470102\n",
      "-34.1301553369365\n",
      "Iteration :  11\n",
      "-10.963897298324927\n",
      "-32.19709011098835\n",
      "Iteration :  12\n",
      "-13.688188599558211\n",
      "-30.580609376942775\n",
      "Iteration :  13\n",
      "-15.980606159728787\n",
      "-29.22716832658185\n",
      "Iteration :  14\n",
      "-17.908980554016537\n",
      "-28.092897239717374\n",
      "Iteration :  15\n",
      "-19.530727378660504\n",
      "-27.141634675504964\n",
      "Iteration :  16\n",
      "-20.89435566812407\n",
      "-26.34343276145494\n",
      "Iteration :  17\n",
      "-22.040793150192002\n",
      "-25.673400022649897\n",
      "Iteration :  18\n",
      "-23.004536017690562\n",
      "-25.110790831207982\n",
      "Iteration :  19\n",
      "-23.814637737810923\n",
      "-24.6382789338338\n",
      "Iteration :  20\n",
      "-24.495554150489344\n",
      "-24.2413711736739\n",
      "Iteration :  21\n",
      "-25.06786251250081\n",
      "-23.90792995557803\n",
      "Iteration :  22\n",
      "-25.548871302201274\n",
      "-23.627781420414387\n",
      "Iteration :  23\n",
      "-25.953136152733965\n",
      "-23.392392095889726\n",
      "Iteration :  24\n",
      "-26.29289560250312\n",
      "-23.19460087159769\n",
      "Iteration :  25\n",
      "-26.578438651070996\n",
      "-23.02839607729173\n",
      "Iteration :  26\n",
      "-26.818414498204856\n",
      "-22.888729595350913\n",
      "Iteration :  27\n",
      "-27.020093377017577\n",
      "-22.771361552035074\n",
      "Iteration :  28\n",
      "-27.189586088521537\n",
      "-22.672730366274948\n",
      "Iteration :  29\n",
      "-27.332028704964095\n",
      "-22.58984389562\n",
      "Iteration :  30\n",
      "-27.451737923527126\n",
      "-22.520188178744853\n",
      "Iteration :  31\n",
      "-27.55234170614304\n",
      "-22.461650882558946\n",
      "Iteration :  32\n",
      "-27.63688911947357\n",
      "-22.412457054775867\n",
      "Iteration :  33\n",
      "-27.707942675774362\n",
      "-22.371115185261147\n",
      "Iteration :  34\n",
      "-27.767655955676712\n",
      "-22.336371910387864\n",
      "Iteration :  35\n",
      "-27.817838854507198\n",
      "-22.307173968146813\n",
      "Iteration :  36\n",
      "-27.860012422820216\n",
      "-22.282636238756133\n",
      "Iteration :  37\n",
      "-27.895454959034215\n",
      "-22.26201489449121\n",
      "Iteration :  38\n",
      "-27.925240748547306\n",
      "-22.244684840148572\n",
      "Iteration :  39\n",
      "-27.950272621842537\n",
      "-22.230120757380334\n",
      "Iteration :  40\n",
      "-27.97130931738282\n",
      "-22.217881176479953\n",
      "Iteration :  41\n",
      "-27.988988478025185\n",
      "-22.207595091658163\n",
      "Iteration :  42\n",
      "-28.003845977582642\n",
      "-22.198950713377506\n",
      "Iteration :  43\n",
      "-28.0163321630818\n",
      "-22.19168601636172\n",
      "Iteration :  44\n",
      "-28.026825504874182\n",
      "-22.185580796495067\n",
      "Iteration :  45\n",
      "-28.03564406824965\n",
      "-22.180449995668994\n",
      "Iteration :  46\n",
      "-28.043055154206066\n",
      "-22.176138092133073\n",
      "Iteration :  47\n",
      "-28.049283401558384\n",
      "-22.17251438624539\n",
      "Iteration :  48\n",
      "-28.054517595946756\n",
      "-22.169469038684188\n",
      "Iteration :  49\n",
      "-28.058916392117286\n",
      "-22.16690974100696\n",
      "Iteration :  50\n",
      "-28.062613122914843\n",
      "-22.164758917620468\n",
      "Iteration :  51\n",
      "-28.065719840748283\n",
      "-22.162951374339354\n",
      "Iteration :  52\n",
      "-28.06833071402594\n",
      "-22.161432322251585\n",
      "Iteration :  53\n",
      "-28.07052488150914\n",
      "-22.160155716987497\n",
      "Iteration :  54\n",
      "-28.072368851101135\n",
      "-22.159082863051026\n",
      "Iteration :  55\n",
      "-28.073918515780814\n",
      "-22.15818124090707\n",
      "Iteration :  56\n",
      "-28.07522084778597\n",
      "-22.157423521271532\n",
      "Iteration :  57\n",
      "-28.076315322398564\n",
      "-22.156786736725323\n",
      "Iteration :  58\n",
      "-28.07723511448842\n",
      "-22.15625158554252\n",
      "Iteration :  59\n",
      "-28.07800810408394\n",
      "-22.15580184663062\n",
      "Iteration :  60\n",
      "-28.078657721449773\n",
      "-22.155423887848826\n",
      "Iteration :  61\n",
      "-28.079203657286826\n",
      "-22.155106252800834\n",
      "Iteration :  62\n",
      "-28.07966245958151\n",
      "-22.15483931357719\n",
      "Iteration :  63\n",
      "-28.0800480351955\n",
      "-22.15461497892141\n",
      "Iteration :  64\n",
      "-28.08037207139978\n",
      "-22.154426448973965\n",
      "Iteration :  65\n",
      "-28.080644390130175\n",
      "-22.154268009160155\n",
      "Iteration :  66\n",
      "-28.0808732457023\n",
      "-22.15413485697428\n",
      "Iteration :  67\n",
      "-28.081065575009998\n",
      "-22.15402295640977\n",
      "Iteration :  68\n",
      "-28.08122720779112\n",
      "-22.153928915622853\n",
      "Iteration :  69\n",
      "-28.08136304333405\n",
      "-22.1538498841216\n",
      "Iteration :  70\n",
      "-28.081477198981162\n",
      "-22.153783466363986\n",
      "Iteration :  71\n",
      "-28.08157313493051\n",
      "-22.15372764914609\n",
      "Iteration :  72\n",
      "-28.08165375911872\n",
      "-22.153680740579375\n",
      "Iteration :  73\n",
      "-28.08172151536409\n",
      "-22.15364131880749\n",
      "Iteration :  74\n",
      "-28.08177845744176\n",
      "-22.15360818890804\n",
      "Iteration :  75\n",
      "-28.081826311336133\n",
      "-22.153580346673024\n",
      "Iteration :  76\n",
      "-28.081866527557608\n",
      "-22.15355694817005\n",
      "Iteration :  77\n",
      "-28.08190032510932\n",
      "-22.153537284161718\n",
      "Iteration :  78\n",
      "-28.081928728436626\n",
      "-22.153520758607748\n",
      "Iteration :  79\n",
      "-28.081952598479315\n",
      "-22.153506870598275\n",
      "Iteration :  80\n",
      "-28.081972658767743\n",
      "-22.15349519917065\n",
      "Iteration :  81\n",
      "-28.081989517353918\n",
      "-22.153485390549545\n",
      "Iteration :  82\n",
      "-28.082003685242327\n",
      "-22.15347714742359\n",
      "Iteration :  83\n",
      "-28.08201559187909\n",
      "-22.1534702199335\n",
      "Iteration :  84\n",
      "-28.082025598169015\n",
      "-22.15346439809853\n",
      "Iteration :  85\n",
      "-28.082034007415054\n",
      "-22.153459505451703\n",
      "Iteration :  86\n",
      "-28.082041074511796\n",
      "-22.153455393690873\n",
      "Iteration :  87\n",
      "-28.08204701367164\n",
      "-22.15345193818351\n",
      "Iteration :  88\n",
      "-28.082052004917827\n",
      "-22.153449034188945\n",
      "Iteration :  89\n",
      "-28.082056199541164\n",
      "-22.153446593683523\n",
      "Iteration :  90\n",
      "-28.082059724685838\n",
      "-22.153444542692526\n",
      "Iteration :  91\n",
      "-28.082062687203326\n",
      "-22.153442819047893\n",
      "Iteration :  92\n",
      "-28.082065176891177\n",
      "-22.153441370503835\n",
      "Iteration :  93\n",
      "-28.082067269214892\n",
      "-22.153440153153202\n",
      "Iteration :  94\n",
      "-28.082069027595377\n",
      "-22.1534391300966\n",
      "Iteration :  95\n",
      "-28.082070505331306\n",
      "-22.15343827032392\n",
      "Iteration :  96\n",
      "-28.08207174721467\n",
      "-22.1534375477744\n",
      "Iteration :  97\n",
      "-28.082072790888482\n",
      "-22.15343694054667\n",
      "Iteration :  98\n",
      "-28.082073667987782\n",
      "-22.153436430234915\n",
      "Iteration :  99\n",
      "-28.082074405098524\n",
      "-22.153436001370956\n",
      "-28.082074405098524\n",
      "-22.153436001370956\n"
     ]
    }
   ],
   "source": [
    "gd = MeroRegression(0.01,100)\n",
    "gd.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b22b00-9ba6-4dc1-b4f2-21bf92b1f27e",
   "metadata": {},
   "source": [
    "### Within 100 iteration,our b and M at first maintains within their starting value i.e. -120 and 100 and after iteration 42 it maintains a constant value i.e. b as -22 and m as -28 which is very close to its original value as predicted by linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9aa74fa0-ffa7-40be-81c1-69455a575db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b5266997-e8a8-4523-95cd-b707c6a35821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2943540520485939"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(Y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab37c1a-6a3b-4d1b-940f-ac57a1ef9304",
   "metadata": {},
   "source": [
    "### Here r2_score is also quite similar as of calculated by LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf25c97-ebf8-48d1-908f-80f3f51d15f7",
   "metadata": {},
   "source": [
    "### If r2_score comes negative then it signifies that our model is poorely predicting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d87686e-6563-4c5a-a34d-69709ba59ee6",
   "metadata": {},
   "source": [
    "# Here when starting with a random value of b and m,our line is at random position in term of both angle(m) and height(b).So on each epoch when we adjust b and m using gradient descent,then our line gets fit into its most optimal position interm of both height and angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83288b-7373-4075-ba17-30d0a6967911",
   "metadata": {},
   "source": [
    "<img src = 'animation4.gif'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5917f92f-3862-4a0a-98e5-2b73140d5414",
   "metadata": {},
   "source": [
    "# Here on second picture,on first epoch our cost function is high and as epoch increases,our cost function gradually goes on decreasing and maintains a most minimum cost function after cetain epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4ece3-cb03-49aa-9034-6344bb4db588",
   "metadata": {},
   "source": [
    "<img src='animation3.gif' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d929c1f-c479-445f-82ca-1519ed21d6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
