{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d79af46-1bbd-4320-a7bd-4786aca42d08",
   "metadata": {},
   "source": [
    "### Gradient descent is a algorithm that is used to find the best fittest line among a multiple choice for training dataset.Best fitted line minimizes the distance between the actual data points and predicted values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5b2cac-723a-4f4a-bfe7-283055b0f7f2",
   "metadata": {},
   "source": [
    "<img src = \"GD.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37765f57-9d44-47e1-9b2f-53f19b90e333",
   "metadata": {},
   "source": [
    "### In above figure,red dots represents the actual data point loaded in model whereas corresponding straight dot in blue line is the predicted value by model.Blue line is the best fitted line.The distance between predicted and actual data point is represented by Δ ,difference between 1st predicted and actual data point is represented by Δ1,2nd by Δ2 and so on. Formula for calculating Avearage error also known as mean square error is given in the right side of the screen where n represents a number of data points.Summarizing it in a more simple way,mean square error can be calculated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702dd96c-5a8d-42fc-9c4d-f1eeca12b8b6",
   "metadata": {},
   "source": [
    "<img src =\"MSE.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0346d-ad19-48d6-93a2-1d8ee9ac9b6a",
   "metadata": {},
   "source": [
    "### Mean square error is also called as  Cost function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a42b5d6-aedb-4b8f-ad9f-57cbb4a56abb",
   "metadata": {},
   "source": [
    "<img src= \"MSE(2).png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e81f83b-e24d-4ba7-9e63-f9139998a386",
   "metadata": {},
   "source": [
    "### Since predicted value depends on the input value of Xi,so we replace it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a1f44-2b95-458e-918d-97113f56a9c1",
   "metadata": {},
   "source": [
    "## Lets understand the difference between partial derivative and simple derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929905d8-f6c0-4852-86f0-b5b56150a54d",
   "metadata": {},
   "source": [
    "### Derivative is done when we have a single variable.For eg : y = 2x+ 3,then dy/dx = 2 <br><br> Partial derivative is done when we have more than one variable in a equation,for eg 3a + 2b -3.So at this case at first we have to do partial derivation(one at a time either x or y).At first lets do partial derivative of 'a' keeping 'b' constant.so dy/da=3 and lets do of0'b' keeping 'a'constant dy/db=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829a6cb-3e81-4e2b-b875-f399df98a99b",
   "metadata": {},
   "source": [
    "## Now lets perform partial derivative of MSE wrt to both \"m\" and  \"b\" as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b76f7-b8be-470b-b83e-153536901289",
   "metadata": {},
   "source": [
    "<img src= \"PD.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8339c-3a29-4208-9c1e-5a46466bc2cd",
   "metadata": {},
   "source": [
    "# Actual implementation of Gradient descent,there are following steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93e196-b00a-427c-8924-6b86c0d2e7ad",
   "metadata": {},
   "source": [
    "### 1) At first choose a random value for m and b <br> 2) Then calculate y_predicted for a given random value of m and b using \"y_pred = m*X + b\" <br>3) Then using above formula,calculate partial derivation with respect to m and b, <br>4)Then calculate new b and m by substracting as \"b = b - n*d/db\",\"m = m - n * d/dm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6fc1fdd-df6f-413b-a3c1-5b20679cbb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost function : 232626.0 , b : 99.096, m :116.812, iteration : 0\n",
      "cost function : 221775.345712 , b : 98.212936, m :113.69955999999999, iteration : 1\n",
      "cost function : 211432.09174491867 , b : 97.350312768, m :110.660892064, iteration : 2\n",
      "cost function : 201572.510082512 , b : 96.50764679007999, m :107.69425056198399, iteration : 3\n",
      "cost function : 192173.98232362562 , b : 95.68446599312793, m :104.79793116887987, iteration : 4\n",
      "cost function : 183214.94779224147 , b : 94.8803094741284, m :101.97026988720575, iteration : 5\n",
      "cost function : 174674.85407410187 , b : 94.09472723585691, m :99.20964209284244, iteration : 6\n",
      "cost function : 166534.1098664356 , b : 93.32727992882813, m :96.51446160338477, iteration : 7\n",
      "cost function : 158774.04003261554 , b : 92.57753859935016, m :93.88317976853735, iteration : 8\n",
      "cost function : 151376.8427586361 , b : 91.84508444354024, m :91.31428458203342, iteration : 9\n",
      "cost function : 144325.54871312308 , b : 91.12950856716097, m :88.80629981456745, iteration : 10\n",
      "cost function : 137603.98211718042 , b : 90.43041175113925, m :86.357784167244, iteration : 11\n",
      "cost function : 131196.7236347643 , b : 89.7474042226335, m :83.9673304450578, iteration : 12\n",
      "cost function : 125089.07499844872 , b : 89.0801054315179, m :81.63356474993073, iteration : 13\n",
      "cost function : 119267.02528942896 , b : 88.42814383215527, m :79.35514569284314, iteration : 14\n",
      "cost function : 113717.21879440421 , b : 87.7911566703339, m :77.13076362460765, iteration : 15\n",
      "cost function : 108426.92436559881 , b : 87.16878977524559, m :74.95913988484428, iteration : 16\n",
      "cost function : 103384.0062136286 , b : 86.56069735638603, m :72.83902606872623, iteration : 17\n",
      "cost function : 98576.89606620782 , b : 85.9665418052609, m :70.76920331107593, iteration : 18\n",
      "cost function : 93994.56662882365 , b : 85.38599350178393, m :68.74848158740069, iteration : 19\n",
      "cost function : 89626.50628649404 , b : 84.81873062525595, m :66.77569903146717, iteration : 20\n",
      "cost function : 85462.69498857037 , b : 84.26443896981664, m :64.84972126902336, iteration : 21\n",
      "cost function : 81493.58126126135 , b : 83.72281176426286, m :62.969440767285946, iteration : 22\n",
      "cost function : 77710.06029514146 , b : 83.19354949613061, m :61.133776199820076, iteration : 23\n",
      "cost function : 74103.4530573736 , b : 82.67635973993943, m :59.341671826447254, iteration : 24\n",
      "cost function : 70665.48638072568 , b : 82.17095698950087, m :57.592096887825775, iteration : 25\n",
      "cost function : 67388.27398370346 , b : 81.67706249419491, m :55.8840450143566, iteration : 26\n",
      "cost function : 64264.298378255975 , b : 81.19440409912039, m :54.216533649075586, iteration : 27\n",
      "cost function : 61286.3936235478 , b : 80.7227160890277, m :52.5886034842012, iteration : 28\n",
      "cost function : 58447.72888623255 , b : 80.26173903594443, m :50.99931791101461, iteration : 29\n",
      "cost function : 55741.792769512394 , b : 79.81121965040646, m :49.447762482756616, iteration : 30\n",
      "cost function : 53162.37837503231 , b : 79.3709106362091, m :47.93304439023353, iteration : 31\n",
      "cost function : 50703.5690633387 , b : 78.94057054859529, m :46.45429194983114, iteration : 32\n",
      "cost function : 48359.72488023474 , b : 78.51996365579912, m :45.01065410364328, iteration : 33\n",
      "cost function : 46125.46961789269 , b : 78.10885980386566, m :43.601299931428336, iteration : 34\n",
      "cost function : 43995.678481039315 , b : 77.70703428466935, m :42.22541817411372, iteration : 35\n",
      "cost function : 41965.46632991879 , b : 77.31426770705532, m :40.882216768575205, iteration : 36\n",
      "cost function : 40030.176473060856 , b : 76.93034587102976, m :39.57092239342422, iteration : 37\n",
      "cost function : 38185.36998414296 , b : 76.55505964492716, m :38.29078002554271, iteration : 38\n",
      "cost function : 36426.81551843805 , b : 76.18820484548405, m :37.0410525071112, iteration : 39\n",
      "cost function : 34750.47960548489 , b : 75.82958212075042, m :35.821020122881855, iteration : 40\n",
      "cost function : 33152.51739571148 , b : 75.47899683577162, m :34.62998018745395, iteration : 41\n",
      "cost function : 31629.263839782485 , b : 75.13625896097535, m :33.46724664231534, iteration : 42\n",
      "cost function : 30177.225280435017 , b : 74.80118296319951, m :32.33214966241855, iteration : 43\n",
      "cost function : 28793.071437513165 , b : 74.4735876992986, m :31.224035272066146, iteration : 44\n",
      "cost function : 27473.62776781371 , b : 74.15329631226761, m :30.1422649698849, iteration : 45\n",
      "cost function : 26215.86818221548 , b : 73.84013612982378, m :29.086215362673826, iteration : 46\n",
      "cost function : 25016.90810338421 , b : 73.53393856538808, m :28.05527780791606, iteration : 47\n",
      "cost function : 23873.997848126422 , b : 73.23453902140982, m :27.048858064749577, iteration : 48\n",
      "cost function : 22784.516319210386 , b : 72.9417767949785, m :26.066375953196626, iteration : 49\n",
      "cost function : 21745.964992182224 , b : 72.65549498566936, m :25.107265021456428, iteration : 50\n",
      "cost function : 20755.962183382126 , b : 72.37554040556928, m :24.17097222107037, iteration : 51\n",
      "cost function : 19812.237586010564 , b : 72.10176349143173, m :23.256957589773407, iteration : 52\n",
      "cost function : 18912.62706170954 , b : 71.83401821891022, m :22.3646939418498, iteration : 53\n",
      "cost function : 18055.06767570985 , b : 71.5721620188213, m :21.493666565815644, iteration : 54\n",
      "cost function : 17237.592964154406 , b : 71.31605569538877, m :20.643372929254774, iteration : 55\n",
      "cost function : 16458.328422739985 , b : 71.06556334642246, m :19.813322390638834, iteration : 56\n",
      "cost function : 15715.487206327904 , b : 70.82055228538579, m :19.003035917966244, iteration : 57\n",
      "cost function : 15007.366029657665 , b : 70.58089296530721, m :18.212045814058673, iteration : 58\n",
      "cost function : 14332.341259759356 , b : 70.34645890449225, m :17.43989544835754, iteration : 59\n",
      "cost function : 13688.865191100163 , b : 70.11712661399312, m :16.68613899506672, iteration : 60\n",
      "cost function : 13075.462494919619 , b : 69.89277552679474, m :15.950341177491294, iteration : 61\n",
      "cost function : 12490.72683460791 , b : 69.6732879286762, m :15.232077018425718, iteration : 62\n",
      "cost function : 11933.317639362323 , b : 69.4585488907083, m :14.530931596448294, iteration : 63\n",
      "cost function : 11401.957028720177 , b : 69.2484462033482, m :13.846499807982182, iteration : 64\n",
      "cost function : 10895.426880912673 , b : 69.04287031209361, m :13.178386134986486, iteration : 65\n",
      "cost function : 10412.566038314015 , b : 68.84171425465951, m :12.526204418144221, iteration : 66\n",
      "cost function : 9952.267643574662 , b : 68.64487359964133, m :11.88957763541709, iteration : 67\n",
      "cost function : 9513.47660032747 , b : 68.45224638662954, m :11.268137685840067, iteration : 68\n",
      "cost function : 9095.187152641161 , b : 68.26373306774124, m :10.661525178431809, iteration : 69\n",
      "cost function : 8696.440577668034 , b : 68.07923645053516, m :10.069389226099862, iteration : 70\n",
      "cost function : 8316.32298619252 , b : 67.89866164227749, m :9.491387244422453, iteration : 71\n",
      "cost function : 7953.963226034728 , b : 67.7219159955264, m :8.927184755191494, iteration : 72\n",
      "cost function : 7608.5308834990465 , b : 67.5489090550042, m :8.376455194604123, iteration : 73\n",
      "cost function : 7279.23437828285 , b : 67.37955250572656, m :7.838879725992807, iteration : 74\n",
      "cost function : 6965.31914747475 , b : 67.21376012235916, m :7.314147056986606, iteration : 75\n",
      "cost function : 6666.065914476215 , b : 67.05144771977253, m :6.801953260998745, iteration : 76\n",
      "cost function : 6380.789038875181 , b : 66.89253310476698, m :6.302001602938137, iteration : 77\n",
      "cost function : 6108.834943486079 , b : 66.73693602893982, m :5.814002369044896, iteration : 78\n",
      "cost function : 5849.580614947614 , b : 66.58457814266768, m :5.33767270075227, iteration : 79\n",
      "cost function : 5602.432174438476 , b : 66.43538295017783, m :4.872736432479714, iteration : 80\n",
      "cost function : 5366.823515231992 , b : 66.28927576568259, m :4.418923933264093, iteration : 81\n",
      "cost function : 5142.215003964086 , b : 66.14618367055164, m :3.9759719521381873, iteration : 82\n",
      "cost function : 4928.092242634991 , b : 66.00603547149771, m :3.5436234671678375, iteration : 83\n",
      "cost function : 4723.964888504684 , b : 65.86876165975171, m :3.121627538061159, iteration : 84\n",
      "cost function : 4529.365529174577 , b : 65.73429437120384, m :2.709739162265303, iteration : 85\n",
      "cost function : 4343.848610274842 , b : 65.60256734748785, m :2.3077191344682433, iteration : 86\n",
      "cost function : 4166.98941329726 , b : 65.47351589798606, m :1.9153339094250148, iteration : 87\n",
      "cost function : 3998.383081228626 , b : 65.34707686273354, m :1.5323554680297482, iteration : 88\n",
      "cost function : 3837.64368974935 , b : 65.2231885761999, m :1.1585611865566925, iteration : 89\n",
      "cost function : 3684.403361866458 , b : 65.10179083192816, m :0.7937337089952458, iteration : 90\n",
      "cost function : 3538.3114239498245 , b : 64.98282484801032, m :0.4376608224057814, iteration : 91\n",
      "cost function : 3399.033601235466 , b : 64.86623323337987, m :0.09013533522479222, iteration : 92\n",
      "cost function : 3266.2512509502467 , b : 64.75195995490176, m :-0.24904504155043244, iteration : 93\n",
      "cost function : 3139.6606312986833 , b : 64.63995030524126, m :-0.5800778103657336, iteration : 94\n",
      "cost function : 3018.9722046347983 , b : 64.53015087149296, m :-0.9031558003691351, iteration : 95\n",
      "cost function : 2903.9099732203913 , b : 64.4225095045522, m :-1.218467277989972, iteration : 96\n",
      "cost function : 2794.2108460458476 , b : 64.31697528921103, m :-1.5261960549015057, iteration : 97\n",
      "cost function : 2689.6240352609016 , b : 64.21349851496201, m :-1.8265215934289387, iteration : 98\n",
      "cost function : 2589.9104808306547 , b : 64.11203064749266, m :-2.119619109463274, iteration : 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_14676\\4290728980.py:17: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  cost_function = (1/n) * np.sum(val**2 for val in (y - y_pred)) # (val**2 for val in (y - y_pred)) this expression means we for each result from\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "X = np.array([1,2,3,4,5])\n",
    "Y = np.array([6,7,8,9,10])\n",
    "\n",
    "\n",
    "def GD(x,y):\n",
    "    b = 100 ##randomly calculated value\n",
    "    m = 120 ## randomly calculated value\n",
    "    iteration = 100\n",
    "    learning_rate = 0.001\n",
    "    n = len(x)\n",
    "    for i in range(iteration):\n",
    "        y_pred = m * x + b\n",
    "        cost_function = (1/n) * np.sum(val**2 for val in (y - y_pred)) # (val**2 for val in (y - y_pred)) this expression means we for each result from \n",
    "        # y-y_pred we are performing their square and adding all the values\n",
    "        partial_b = (2/n) * np.sum(-(y - y_pred))\n",
    "        partial_m = (2/n) * np.sum(-x*(y - y_pred))\n",
    "        b = b-learning_rate*partial_b\n",
    "        m = m - learning_rate * partial_m\n",
    "        print(\"cost function : {} , b : {}, m :{}, iteration : {}\".format(cost_function,b,m,i))\n",
    "        \n",
    "\n",
    "GD(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a775cab8-fed7-4a7a-900a-0b35c76dfe3f",
   "metadata": {},
   "source": [
    "## In above with eac iteration cost function is being decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b1d62-6459-41fe-8348-f90c51a3bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "X = np.array([1,2,3,4,5])\n",
    "Y = np.array([6,7,8,9,10])\n",
    "\n",
    "\n",
    "def GD(x,y):\n",
    "    b = 100 ##randomly calculated value\n",
    "    m = 120 ## randomly calculated value\n",
    "    iteration = 100\n",
    "    learning_rate = 0.001\n",
    "    n = len(x)\n",
    "    for i in range(iteration):\n",
    "        y_pred = m * x + b\n",
    "        cost_function = (1/n) * np.sum(val**2 for val in (y - y_pred)) # (val**2 for val in (y - y_pred)) this expression means we for each result from \n",
    "        # y-y_pred we are performing their square and adding all the values\n",
    "        partial_b = (2/n) * np.sum(-(y - y_pred))\n",
    "        partial_m = (2/n) * np.sum(-x*(y - y_pred))\n",
    "        b = b-learning_rate*partial_b\n",
    "        m = m - learning_rate * partial_m\n",
    "        print(\"cost function : {} , b : {}, m :{}, iteration : {}\".format(cost_function,b,m,i))\n",
    "        \n",
    "\n",
    "GD(X,Y)\n",
    "\n",
    "working of above code :\n",
    "y_pred = 120*[1,2,3,4,5] + 100\n",
    "y_pred = [220, 340, 460, 580, 700]\n",
    "cost_function = 1/5 * [(6-120)^2 +(7-340)^2 + (8-460)^2 + (9-580)^2 + (10-700)^2]\n",
    "partial_b = 2/5 * ([(6-120) +(7-340) + (8-460) + (9-580) + (10-700)])\n",
    "partial_m = 2/5 * np.sum(-[1,2,3,4,5]*([(6-120),(7-340), (8-460), (9-580), (10-700)])) = 2/5 * np.sum([214, 666, 1356, 2284, 3450])\n",
    "          = \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
